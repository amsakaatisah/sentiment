{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "support_vector_machine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amsakaatisah/sentiment/blob/main/support_vector_machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opSAqiIf7IXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b54e428-0621-43e7-de2c-7e84b1a858cb"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from wordcloud import WordCloud,STOPWORDS\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import spacy\r\n",
        "import re,string,unicodedata\r\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\r\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.svm import SVC\r\n",
        "from textblob import TextBlob\r\n",
        "from textblob import Word\r\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\r\n",
        "\r\n",
        "import os\r\n",
        "print(os.listdir(\"/content/drive/MyDrive/Colab Notebooks\"))\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['20191002-reviews.csv', '20191002-items.csv', 'IMDB Dataset.csv', 'Salinan dari Data eksternal: File Lokal, Drive, Spreadsheet, dan Cloud Storage', 'Untitled0.ipynb', '.ipynb_checkpoints', 'imdb-dataset-of-50k-movie-reviews.zip', 'Dataset', 'NLTK.ipynb', 'video-game-sales-with-ratings.zip', '1_Logistic_Regression.ipnyb', 'Logistic_Regression.ipynb', 'Untitled1.ipynb', 'stopwordbahasa.csv', 'Untitled2.ipynb', 'Salinan 20191002-reviews.csv', 'Salinan 20191002-items.csv', 'Dataset Indonesia', 'Logistic Regression(IMDB).ipynb', 'Support Vector Machine.ipynb', 'Untitled3.ipynb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9t4kj9AKX39",
        "outputId": "9394dc65-6a58-4118-ae60-79933c4a26e4"
      },
      "source": [
        "# Setup\r\n",
        "!pip install -q wordcloud\r\n",
        "import wordcloud\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger') \r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import io\r\n",
        "import unicodedata\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcbzNlC_cq6e"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IMDB Dataset.csv').iloc[:20000, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz6xTJH4aW2v",
        "outputId": "82bc8121-9786-4a17-e83b-5a6ed97c512d"
      },
      "source": [
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "corpus = []\r\n",
        "for i in range(0, len(dataset)):\r\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['review'][i])\r\n",
        "    review = review.lower()\r\n",
        "    review = review.split()\r\n",
        "    ps = PorterStemmer()\r\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\r\n",
        "    review = ' '.join(review)\r\n",
        "    corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzm9h4uZflMR"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "cv = CountVectorizer(max_features = 1000)\r\n",
        "X = cv.fit_transform(corpus).toarray()\r\n",
        "y = dataset.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP-pb6Uqfonw"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "le = LabelEncoder()\r\n",
        "y = le.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv6ScH_Dfqxb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inBXJvsSfsiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ff77d1-a078-4896-d27a-5afbb078f6c0"
      },
      "source": [
        "from sklearn.svm import SVC\r\n",
        "model = SVC(kernel='linear')\r\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUCZGE5pfuGn"
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN46NlhnfwrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3b7e9d-9442-4f91-e7d6-a9f51d236dba"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_eXQLjOfyN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce9835c-59a2-4536-8250-3e49767544ec"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.863\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}